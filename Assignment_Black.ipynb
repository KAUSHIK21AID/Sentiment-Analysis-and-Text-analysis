{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHqKwJViC2ia",
        "outputId": "42bb8ebf-ab39-44bd-ea85-e05d6857cc31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import glob\n",
        "from google.colab import drive\n",
        "glob.glob('https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/edit?usp=share_link&ouid=112098211784036012178&rtpof=true&sd=true')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELgg6izYyJss",
        "outputId": "bee401bf-7c29-4720-dda9-070f80308ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trafilatura\n",
            "  Downloading trafilatura-1.6.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from trafilatura) (2023.7.22)\n",
            "Collecting courlan>=0.9.4 (from trafilatura)\n",
            "  Downloading courlan-0.9.4-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting htmldate>=1.5.1 (from trafilatura)\n",
            "  Downloading htmldate-1.5.2-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting justext>=3.0.0 (from trafilatura)\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from trafilatura) (4.9.3)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from trafilatura) (3.3.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from trafilatura) (2.0.6)\n",
            "Requirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.9.4->trafilatura) (3.3.0)\n",
            "Collecting tld>=0.13 (from courlan>=0.9.4->trafilatura)\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dateparser>=1.1.2 (from htmldate>=1.5.1->trafilatura)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.5.1->trafilatura) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2023.3.post1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2023.6.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.5.1->trafilatura) (1.16.0)\n",
            "Installing collected packages: tld, justext, dateparser, courlan, htmldate, trafilatura\n",
            "Successfully installed courlan-0.9.4 dateparser-1.1.8 htmldate-1.5.2 justext-3.0.0 tld-0.13 trafilatura-1.6.2\n"
          ]
        }
      ],
      "source": [
        "pip install trafilatura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTS8AaC1C_ef",
        "outputId": "7b7fa447-997c-4cfc-899d-a4984bfe2004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "52\n",
            "[63, 33, 0.31249999674479173, 0.10631229224107167, 25.176470588235293, 19.439252336448597, 10.148345244639913, 25.176470588235293, 416, 903, 1.6948598130841122, 9, 4.941588785046729, 2140]\n",
            "[31, 11, 0.476190464852608, 0.030589948994471993, 12.582995951417004, 18.693693693693696, 5.107973155341576, 12.582995951417004, 581, 1373, 1.6763191763191763, 7, 5.148648648648648, 3108]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import trafilatura\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "''' As the file is mounted in my drive, the code successfully gets executed only in my colab '''\n",
        "''' Or please download the file and copy the path of Input, Output, Stopwords, Negative, Positive txt files '''\n",
        "df = pd.read_excel(r'/content/drive/MyDrive/BlackCoffer/Input.xlsx') #reading the Input Excel file given\n",
        "f=df.URL # Storing all the given URL's in a list\n",
        "number=1\n",
        "row1 = int(input())\n",
        "row2 = int(input())\n",
        "listdataframe=[]\n",
        "for link in f:\n",
        "    [positive_score,negative_score,polarity_score,subjectivity_score,percentage_complex_words,fog_index,average_sentence_length,count_complex_words,No_of_words_analysis,Average_syllable,c,average_word_length,No_of_words_after_cleaning,dd]=['','','','','','','','','','','','','','']\n",
        "    flag=100\n",
        "    #checking whether URL has content or not\n",
        "    from urllib.request import urlopen\n",
        "    from urllib.error import *\n",
        "    try:\n",
        "        html = urlopen(link)\n",
        "    except HTTPError as e:\n",
        "        flag=1000\n",
        "    except URLError as e:\n",
        "        flag=1000\n",
        "    else:\n",
        "        active_link=link\n",
        "    if(flag==100): # As flag doesnt change if link has contents\n",
        "        downloaded = trafilatura.fetch_url(active_link)\n",
        "        black=trafilatura.extract(downloaded) #Main content of the webpage is stored as a string in 'black'\n",
        "        Number_of_sentences=black.count('.') #counting the number of full stops hence number of sentences\n",
        "        black=black.replace('\\n',' ')\n",
        "        new_string = black.translate(str.maketrans('', '', string.punctuation)) #removing all the punctuations.\n",
        "        new_string = new_string.replace('“', '')\n",
        "        new_string = new_string.replace('”', '')\n",
        "        pattern = r'[0-9]'\n",
        "        newest_string = re.sub(pattern, '', new_string) #removing all the numbers\n",
        "        newest_string = newest_string.replace(\"’s\",'')\n",
        "        all_words=newest_string.split() #converting the string into a list, all words are individual elements of list named 'all_words'\n",
        "        words_lowered=[]\n",
        "        for k in all_words:\n",
        "            words_lowered.append(k.lower()) #lowering every word\n",
        "        No_of_words_before_cleaning=len(words_lowered)\n",
        "        with open('/content/drive/MyDrive/BlackCoffer/Stop_Words.txt') as f: #all stopwords given are read\n",
        "            stop_words_text = f.read() #stop words are stored in a string\n",
        "        stop_words_final=stop_words_text.split(' ') #converted into a list\n",
        "        stopwords_final_lowered=[]\n",
        "        for t in stop_words_final:\n",
        "            stopwords_final_lowered.append(t.lower()) #all stop words are lowered and stored in a list named \"stopwords_final_lowered\"\n",
        "        words_for_analysis=[]\n",
        "        for g in words_lowered:\n",
        "            if g not in stopwords_final_lowered: #stop words are removed from our main text\n",
        "                words_for_analysis.append(g)\n",
        "        No_of_words_analysis=len(words_for_analysis)\n",
        "        with open('/content/drive/MyDrive/BlackCoffer/Negative_Words.txt') as g: # reading all the negative words given\n",
        "            negative_string=g.read() #negative words are stored as a string\n",
        "        negative_words=negative_string.split(' ') #converted into a list\n",
        "        with open('/content/drive/MyDrive/BlackCoffer/Positive_Words.txt') as h: # reading all the positive words given\n",
        "            positive_string=h.read() #positive words are stored as a string\n",
        "        positive_words=positive_string.split(' ') #converted into a list\n",
        "        positive_in_ours=[]\n",
        "        negative_in_ours=[]\n",
        "        for v in words_for_analysis:\n",
        "            if v in positive_words:\n",
        "                positive_in_ours.append(v) #words which are positive are appended\n",
        "            if v in negative_words:\n",
        "                negative_in_ours.append(v) #words which are negative are appended\n",
        "        positive_score=len(positive_in_ours) #calculation of positive score\n",
        "        negative_score=len(negative_in_ours) #calculation of negative score\n",
        "        polarity_score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001) #calculation of polarity score\n",
        "        if(abs(polarity_score)<0.45):\n",
        "          dd = \"Negative\"\n",
        "        elif(abs(polarity_score)>=0.45 and abs(polarity_score)<=0.55):\n",
        "          dd=\"Neither Positive nor Negative\"\n",
        "        else:\n",
        "          dd=\"Positive\"\n",
        "        subjectivity_score=(positive_score+negative_score)/((No_of_words_analysis)+0.000001) #calculation of subjectivity score\n",
        "        average_sentence_length=No_of_words_before_cleaning/Number_of_sentences #calculation of Average sentence length i.e average words per sentence\n",
        "        total_syllable_count=[]\n",
        "        for word in words_lowered:\n",
        "            def syllable_count(word):\n",
        "                count = 0\n",
        "                vowels = \"aeiouy\" # vowels are stored in as a string\n",
        "                if word[0] in vowels:\n",
        "                    count += 1 #if the first letter of the word is a vowel, then it is counted as a syllable\n",
        "                for index in range(1, len(word)):\n",
        "                    if word[index] in vowels and word[index - 1] not in vowels:\n",
        "                        count += 1 #if a particular letter of a word is a vowel and its previous one isnt a vowel, then it is counted as a syllable\n",
        "                if word.endswith(\"e\"):\n",
        "                    count -= 1 #if word ends with \"e\"\n",
        "                if (word.endswith(\"es\") and word.endswith(\"ed\")):\n",
        "                    count -= 1\n",
        "                if count == 0:\n",
        "                    count += 1 #no word can have zero syllable\n",
        "                return count\n",
        "            count_word=syllable_count(word) #function call and number of syllable is stored in \"count_word\"\n",
        "            total_syllable_count.append(count_word) # appended into a list\n",
        "        count_complex_words=0\n",
        "        for i in total_syllable_count:\n",
        "            if i>2: #if a word has more than 2 syllables it is considered to be a complex word\n",
        "                count_complex_words=count_complex_words+1\n",
        "        No_of_words_after_cleaning=len(words_lowered)\n",
        "        Average_syllable = (sum(total_syllable_count)/No_of_words_after_cleaning) #calculating average syllable per word\n",
        "        percentage_complex_words=count_complex_words/No_of_words_after_cleaning #calculation of percentage of percentage of complex words\n",
        "        fog_index=0.4*(percentage_complex_words+average_sentence_length) #calculation of fog index\n",
        "        c=0\n",
        "        personal_pronouns=['i','we','ours','us','my'] #personal pronouns given\n",
        "        for k in words_lowered:\n",
        "            if k in personal_pronouns:\n",
        "                c=c+1 #number of personal pronouns in the original text before removing stopwords are counted\n",
        "        count_characters=0\n",
        "        listaaa=[]\n",
        "        for k in words_lowered:\n",
        "            listaaa.append(len(k)) #length of each words are appended into a list named \"listaaa\"\n",
        "        count_characters=sum(listaaa) # total number of characters in the original text\n",
        "        average_word_length=count_characters/No_of_words_before_cleaning ##calculation of Average word length\n",
        "    listinputs = [number,link,positive_score,negative_score,polarity_score,subjectivity_score,average_sentence_length,percentage_complex_words*100,fog_index,average_sentence_length,count_complex_words,No_of_words_analysis,Average_syllable,c,average_word_length,No_of_words_after_cleaning,dd] #storing all the outputs into a list\n",
        "    listdataframe.append(listinputs) #appending output list of each webpage into a list\n",
        "\n",
        "\n",
        "    if(number==row1):\n",
        "      x = listinputs.copy()\n",
        "    elif(number==row2):\n",
        "      y = listinputs.copy()\n",
        "    number=number+1\n",
        "# creating data frame\n",
        "del x[0]\n",
        "del x[0]\n",
        "del y[0]\n",
        "del y[0]\n",
        "del x[14]\n",
        "del y[14]\n",
        "print(x)\n",
        "print(y)\n",
        "output_dataframe_copy=pd.DataFrame(listdataframe,columns=['URL ID','URL','POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH','WORDCC','RATINGS'])\n",
        "outputdataframe=pd.DataFrame(listdataframe,columns=['URL ID','URL','POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH','WORDCC','RATINGS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXITl9R-T0Kv",
        "outputId": "bb2c68a9-7aa3-4363-d1fb-c60914aec2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between given two data points : \n",
            "[[1.         0.99958825]\n",
            " [0.99958825 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "matrix = np.corrcoef(x, y)\n",
        "print('Correlation between given two data points : ')\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iFcyX0yLQso"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tafn-5rM9Ti"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/drive/MyDrive/BlackCoffer/output_black.xlsx' #path of the output file\n",
        "\n",
        "outputdataframe.to_excel(file_name) #converting the created data frame into an excel file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QmALvkmD6Vt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbi8i8ZVR_9R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0PZ8HDzSCXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsvhTxfFH6Rw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veSat-oWJpHe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}